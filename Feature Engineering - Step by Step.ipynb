{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, Normalizer, MinMaxScaler, MaxAbsScaler, scale, Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor as KNR\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import kurtosis, skew\n",
    "from string import ascii_letters\n",
    "from functools import partial\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "os.getcwd()\n",
    "sns.set(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: featuretools in /usr/local/envs/py3env/lib/python3.5/site-packages (0.3.1)\n",
      "Requirement already satisfied: s3fs>=0.1.2 in /usr/local/envs/py3env/lib/python3.5/site-packages (from featuretools) (0.1.6)\n",
      "Requirement already satisfied: pyyaml>=3.12 in /usr/local/envs/py3env/lib/python3.5/site-packages (from featuretools) (3.13)\n",
      "Requirement already satisfied: cloudpickle>=0.4.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from featuretools) (0.5.3)\n",
      "Requirement already satisfied: dask>=0.17.5 in /usr/local/envs/py3env/lib/python3.5/site-packages (from featuretools) (0.19.4)\n",
      "Requirement already satisfied: pympler>=0.5 in /usr/local/envs/py3env/lib/python3.5/site-packages (from featuretools) (0.6)\n",
      "Requirement already satisfied: toolz>=0.8.2 in /usr/local/envs/py3env/lib/python3.5/site-packages (from featuretools) (0.9.0)\n",
      "Requirement already satisfied: distributed>=1.21.8 in /usr/local/envs/py3env/lib/python3.5/site-packages (from featuretools) (1.21.8)\n",
      "Requirement already satisfied: pandas>=0.23.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from featuretools) (0.23.4)\n",
      "Requirement already satisfied: tqdm>=4.19.2 in /usr/local/envs/py3env/lib/python3.5/site-packages (from featuretools) (4.26.0)\n",
      "Requirement already satisfied: psutil in /usr/local/envs/py3env/lib/python3.5/site-packages (from featuretools) (4.3.0)\n",
      "Requirement already satisfied: future>=0.16.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from featuretools) (0.16.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/envs/py3env/lib/python3.5/site-packages (from featuretools) (1.14.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/envs/py3env/lib/python3.5/site-packages (from s3fs>=0.1.2->featuretools) (1.9.23)\n",
      "Requirement already satisfied: botocore in /usr/local/envs/py3env/lib/python3.5/site-packages (from s3fs>=0.1.2->featuretools) (1.12.23)\n",
      "Requirement already satisfied: msgpack in /usr/local/envs/py3env/lib/python3.5/site-packages (from distributed>=1.21.8->featuretools) (0.5.6)\n",
      "Requirement already satisfied: tornado>=4.5.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from distributed>=1.21.8->featuretools) (4.5.1)\n",
      "Requirement already satisfied: sortedcontainers in /usr/local/envs/py3env/lib/python3.5/site-packages (from distributed>=1.21.8->featuretools) (2.0.4)\n",
      "Requirement already satisfied: zict>=0.1.3 in /usr/local/envs/py3env/lib/python3.5/site-packages (from distributed>=1.21.8->featuretools) (0.1.3)\n",
      "Requirement already satisfied: tblib in /usr/local/envs/py3env/lib/python3.5/site-packages (from distributed>=1.21.8->featuretools) (1.3.2)\n",
      "Requirement already satisfied: click>=6.6 in /usr/local/envs/py3env/lib/python3.5/site-packages (from distributed>=1.21.8->featuretools) (6.7)\n",
      "Requirement already satisfied: six in /usr/local/envs/py3env/lib/python3.5/site-packages (from distributed>=1.21.8->featuretools) (1.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/envs/py3env/lib/python3.5/site-packages (from pandas>=0.23.0->featuretools) (2.5.0)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/envs/py3env/lib/python3.5/site-packages (from pandas>=0.23.0->featuretools) (2016.7)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/envs/py3env/lib/python3.5/site-packages (from boto3->s3fs>=0.1.2->featuretools) (0.9.3)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in /usr/local/envs/py3env/lib/python3.5/site-packages (from boto3->s3fs>=0.1.2->featuretools) (0.1.13)\n",
      "Requirement already satisfied: urllib3<1.24,>=1.20 in /usr/local/envs/py3env/lib/python3.5/site-packages (from botocore->s3fs>=0.1.2->featuretools) (1.22)\n",
      "Requirement already satisfied: docutils>=0.10 in /usr/local/envs/py3env/lib/python3.5/site-packages (from botocore->s3fs>=0.1.2->featuretools) (0.14)\n",
      "Requirement already satisfied: heapdict in /usr/local/envs/py3env/lib/python3.5/site-packages (from zict>=0.1.3->distributed>=1.21.8->featuretools) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install featuretools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import featuretools as ft\n",
    "from io import BytesIO as BI\n",
    "%matplotlib inline\n",
    "# import all data\n",
    "# first X rows are taken for faster calculations, substitute this by whole dataset\n",
    "\n",
    "bureau = pd.read_csv('OneHot-bureau.csv')\n",
    "\n",
    "app_joint = pd.read_csv('OneHot-application_joint.csv')\n",
    "\n",
    "bureau_balance = pd.read_csv('OneHot-bureau_balance.csv')\n",
    "\n",
    "cre_balance = pd.read_csv('OneHot-credit_card_payments.csv')\n",
    "\n",
    "ins_payments = pd.read_csv('OneHot-installments_payments.csv')\n",
    "\n",
    "pre_application = pd.read_csv('OneHot-previous_application.csv')\n",
    "\n",
    "POS_balance = pd.read_csv('OneHot-POS_CASH_balance.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous = pre_application.copy()\n",
    "cash = POS_balance.copy()\n",
    "installments = ins_payments.copy()\n",
    "credit = cre_balance.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can draw samples based on different training windows. For example, 1 month, 3 months, 6 months, 1 year, 3 years, 100 years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the bureau_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: bureau_data\n",
       "  Entities:\n",
       "    bureau [Rows: 3022, Columns: 31]\n",
       "  Relationships:\n",
       "    No relationships"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize entityset\n",
    "es = ft.EntitySet('bureau_data')\n",
    "\n",
    "today = pd.to_datetime('2018-06-11')\n",
    "bureau['DAYS_CREDIT'] = today + pd.to_timedelta(bureau['DAYS_CREDIT'], unit='d')\n",
    "\n",
    "# add entities (brueau table)\n",
    "es = es.entity_from_dataframe(\n",
    "    entity_id = 'bureau', \n",
    "    dataframe = bureau,\n",
    "    index = 'SK_ID_BUREAU',\n",
    "    time_index = 'DAYS_CREDIT',\n",
    ")\n",
    "\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bureau_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: bureau_data\n",
       "  Entities:\n",
       "    bureau_balance [Rows: 73514, Columns: 12]\n",
       "    bureau [Rows: 3022, Columns: 31]\n",
       "  Relationships:\n",
       "    bureau_balance.SK_ID_BUREAU -> bureau.SK_ID_BUREAU"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if error occurs, try: today = pd.to_datetime('2018-06')\n",
    "bureau_balance['MONTHS_BALANCE'] = today + pd.to_timedelta(bureau_balance['MONTHS_BALANCE'], unit='M')\n",
    "\n",
    "# Entities that do not have a unique index\n",
    "es = es.entity_from_dataframe(\n",
    "    entity_id = 'bureau_balance', \n",
    "    dataframe = bureau_balance,\n",
    "    make_index = True,\n",
    "    index = 'bureaubalance_index',\n",
    "    time_index = 'MONTHS_BALANCE'\n",
    ")\n",
    "\n",
    "r_bureau_balance = ft.Relationship(es['bureau']['SK_ID_BUREAU'], es['bureau_balance']['SK_ID_BUREAU'])\n",
    "es = es.add_relationships([r_bureau_balance])\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 858 ms, sys: 14 ms, total: 872 ms\n",
      "Wall time: 862 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define cut-off times\n",
    "# cut-off times are the \"right\" time values to be used for feature calculation without future leaks\n",
    "# none in our case\n",
    "\n",
    "cutoff_times = pd.DataFrame(bureau.SK_ID_BUREAU)\n",
    "cutoff_times['time'] = today\n",
    "\n",
    "# add last_time_index\n",
    "es.add_last_time_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_matrix(fm_sum_, fm_, period, parent_cols):\n",
    "  child_cols = fm_.drop(columns = parent_cols).columns\n",
    "  fm_ = fm_[child_cols].add_prefix(str(period) + \" MONTHS(\").add_suffix(\")\")\n",
    "  return fm_sum_.join(fm_)\n",
    "  \n",
    "def period_matrix(period, es = es, target_entity = 'prev_apps'):\n",
    "  # Specify the fm with the first period\n",
    "  fm_sum, feature_names = ft.dfs(entityset = es, target_entity = target_entity,\n",
    "                                 drop_contains = ['SK_ID_CURR','SK_ID_PREV','SK_ID_BUREAU'],\n",
    "                                 agg_primitives=[\n",
    "                                   'mean',\n",
    "                                   'count',\n",
    "                                   'last',\n",
    "                                   'num_unique'\n",
    "                                 ], \n",
    "                                 trans_primitives=[\n",
    "                                   'time_since_previous',\n",
    "                                   'weekday',\n",
    "                                   'month',\n",
    "                                   'year'\n",
    "                                 ],\n",
    "                                 max_depth=2,\n",
    "                                 cutoff_time=cutoff_times,\n",
    "                                 training_window=ft.Timedelta(period[0]*31, \"d\"), # use only last X days in computations\n",
    "                                 max_features=3000,\n",
    "                                 chunk_size=10000,\n",
    "                                 verbose=True\n",
    "                                )\n",
    "  parent_cols = [col for col in fm_sum.columns \n",
    "                 if (('(cash.' not in col) & ('(installments.' not in col) & ('(credit.' not in col)\n",
    "                     & ('(bureau_balance.' not in col))]\n",
    "  fm_sum = merge_matrix(fm_sum[parent_cols], fm_sum, period[0], parent_cols)\n",
    "  \n",
    "  for i in period[1:]:\n",
    "    # DFS with specified primitives\n",
    "    fm, feature_names = ft.dfs(entityset = es, target_entity = target_entity,\n",
    "                           drop_contains = ['SK_ID_CURR','SK_ID_PREV','SK_ID_BUREAU'],\n",
    "                           agg_primitives=[\n",
    "                             'mean',\n",
    "                             'count',\n",
    "                             'last',\n",
    "                             'num_unique'\n",
    "                           ], \n",
    "                           trans_primitives=[\n",
    "                             'time_since_previous',\n",
    "                             'weekday',\n",
    "                             'month',\n",
    "                             'year'\n",
    "                           ],\n",
    "                           max_depth=2,\n",
    "                           cutoff_time=cutoff_times,\n",
    "                           training_window=ft.Timedelta(31*i, \"d\"), # use only last X days in computations\n",
    "                           max_features=3000,\n",
    "                           chunk_size=10000,\n",
    "                           verbose=True\n",
    "                          )\n",
    "    fm_sum = merge_matrix(fm_sum, fm, i, parent_cols)\n",
    "    print('%d Total Features' % len(feature_names))\n",
    "  \n",
    "  # Merge with the whole training time matrix\n",
    "  fm, feature_names = ft.dfs(entityset = es, target_entity = target_entity,\n",
    "                       drop_contains = ['SK_ID_CURR','SK_ID_PREV','SK_ID_BUREAU'],\n",
    "                       agg_primitives=[\n",
    "                         'mean',\n",
    "                         'count',\n",
    "                         'last',\n",
    "                         'num_unique'\n",
    "                       ], \n",
    "                       trans_primitives=[\n",
    "                         'time_since_previous',\n",
    "                         'weekday',\n",
    "                         'month',\n",
    "                         'year'\n",
    "                       ],\n",
    "                       max_depth=2,\n",
    "                       cutoff_time=cutoff_times,\n",
    "                       #training_window=ft.Timedelta(31*i, \"d\"), # use only last X days in computations\n",
    "                       max_features=3000,\n",
    "                       chunk_size=10000,\n",
    "                       verbose=True\n",
    "                      )\n",
    "  fm = fm.drop(columns = parent_cols)\n",
    "  fm_sum = fm_sum.join(fm)\n",
    "  return fm_sum, fm_sum.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 59 features\n",
      "Elapsed: 00:00 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 1/1 chunks\n",
      "Built 59 features\n",
      "Elapsed: 00:01 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 1/1 chunks\n",
      "59 Total Features\n",
      "Built 59 features\n",
      "Elapsed: 00:01 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 1/1 chunks\n"
     ]
    }
   ],
   "source": [
    "fm, features_names = period_matrix(period = [6, 36], es = es, target_entity = 'bureau')\n",
    "#features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [col for col in fm.columns if '(LAST(' in col]\n",
    "fm = fm.drop(columns = drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the generated feature_matrix to the original dataframe\n",
    "bureau = bureau.merge(fm.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the previous_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.to_datetime('2018-06-11')\n",
    "previous['DAYS_DECISION'] = today + pd.to_timedelta(previous['DAYS_DECISION'], unit='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize entityset\n",
    "es = ft.EntitySet('previous_data')\n",
    "\n",
    "# add entities (previous applications table)\n",
    "es = es.entity_from_dataframe(\n",
    "    entity_id = 'prev_apps', \n",
    "    dataframe = previous,\n",
    "    index = 'SK_ID_PREV',\n",
    "    time_index = 'DAYS_DECISION'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature for installments\n",
    "installments['DAYS_DELAYS'] = installments['DAYS_ENTRY_PAYMENT'] - installments['DAYS_INSTALMENT']\n",
    "\n",
    "# if error occurs, try: today = pd.to_datetime('2018-06')\n",
    "installments['DAYS_INSTALMENT'] = today + pd.to_timedelta(installments['DAYS_INSTALMENT'], unit='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: previous_data\n",
       "  Entities:\n",
       "    installments [Rows: 22751, Columns: 20]\n",
       "    prev_apps [Rows: 2989, Columns: 148]\n",
       "  Relationships:\n",
       "    installments.SK_ID_PREV -> prev_apps.SK_ID_PREV"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add entities (installments table)\n",
    "es = es.entity_from_dataframe(\n",
    "    entity_id = 'installments', \n",
    "    dataframe = installments,\n",
    "    make_index = True,\n",
    "    time_index = 'DAYS_INSTALMENT',\n",
    "    index = 'installment_index'\n",
    ")\n",
    "\n",
    "r_installments = ft.Relationship(es['prev_apps']['SK_ID_PREV'], es['installments']['SK_ID_PREV'])\n",
    "es = es.add_relationship(r_installments)\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credit_card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: previous_data\n",
       "  Entities:\n",
       "    credit [Rows: 4922, Columns: 27]\n",
       "    installments [Rows: 22751, Columns: 20]\n",
       "    prev_apps [Rows: 2989, Columns: 148]\n",
       "  Relationships:\n",
       "    installments.SK_ID_PREV -> prev_apps.SK_ID_PREV\n",
       "    credit.SK_ID_PREV -> prev_apps.SK_ID_PREV"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = pd.to_datetime('2018-06-11')\n",
    "credit['MONTHS_BALANCE'] = today + pd.to_timedelta(credit['MONTHS_BALANCE'], unit='M')\n",
    "\n",
    "# add entities (credit_card_balance table)\n",
    "es = es.entity_from_dataframe(\n",
    "    entity_id = 'credit', \n",
    "    dataframe = credit,\n",
    "    make_index = True,\n",
    "    index = 'credit_index',\n",
    "    time_index = 'MONTHS_BALANCE'\n",
    ")\n",
    "\n",
    "r_previous_credit = ft.Relationship(es['prev_apps']['SK_ID_PREV'], es['credit']['SK_ID_PREV'])\n",
    "es = es.add_relationship(r_previous_credit)\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### POS_cash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the time variable\n",
    "today = pd.to_datetime('2018-06-11')\n",
    "cash['MONTHS_BALANCE'] = today + pd.to_timedelta(cash['MONTHS_BALANCE'], unit='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: previous_data\n",
       "  Entities:\n",
       "    credit [Rows: 4922, Columns: 27]\n",
       "    cash [Rows: 17821, Columns: 14]\n",
       "    installments [Rows: 22751, Columns: 20]\n",
       "    prev_apps [Rows: 2989, Columns: 148]\n",
       "  Relationships:\n",
       "    installments.SK_ID_PREV -> prev_apps.SK_ID_PREV\n",
       "    credit.SK_ID_PREV -> prev_apps.SK_ID_PREV\n",
       "    cash.SK_ID_PREV -> prev_apps.SK_ID_PREV"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add entities (previous applications table)\n",
    "es = es.entity_from_dataframe(\n",
    "    entity_id = 'cash', \n",
    "    dataframe = cash,\n",
    "    make_index = True,\n",
    "    index = 'cash_index',\n",
    "    time_index = 'MONTHS_BALANCE'\n",
    ")\n",
    "\n",
    "r_cash = ft.Relationship(es['prev_apps']['SK_ID_PREV'], es['cash']['SK_ID_PREV'])\n",
    "es = es.add_relationship(r_cash)\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.52 s, sys: 10.7 ms, total: 2.53 s\n",
      "Wall time: 2.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define cut-off times\n",
    "# cut-off times are the \"right\" time values to be used for feature calculation without future leaks\n",
    "# none in our case\n",
    "\n",
    "cutoff_times = pd.DataFrame(previous.SK_ID_PREV)\n",
    "cutoff_times['time'] = today\n",
    "\n",
    "# add last_time_index\n",
    "es.add_last_time_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 276 features\n",
      "Elapsed: 00:01 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 1/1 chunks\n",
      "Built 276 features\n",
      "Elapsed: 00:01 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 1/1 chunks\n",
      "276 Total Features\n",
      "Built 276 features\n",
      "Elapsed: 00:02 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 1/1 chunks\n"
     ]
    }
   ],
   "source": [
    "fm, features_names = period_matrix(period = [1, 3], es = es, target_entity = 'prev_apps')\n",
    "#features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [col for col in fm.columns if '(LAST(' in col]\n",
    "fm = fm.drop(columns = drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the generated feature_matrix to the original dataframe\n",
    "previous = previous.merge(fm.reset_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Main Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hand Crafted Features\n",
    "app_joint['annuity_income_percentage'] = app_joint['AMT_ANNUITY'] / app_joint['AMT_INCOME_TOTAL']\n",
    "app_joint['car_to_birth_ratio'] = app_joint['OWN_CAR_AGE'] / app_joint['DAYS_BIRTH']\n",
    "app_joint['car_to_employ_ratio'] = app_joint['OWN_CAR_AGE'] / app_joint['DAYS_EMPLOYED']\n",
    "app_joint['children_ratio'] = app_joint['CNT_CHILDREN'] / app_joint['CNT_FAM_MEMBERS']\n",
    "app_joint['credit_to_annuity_ratio'] = app_joint['AMT_CREDIT'] / app_joint['AMT_ANNUITY']\n",
    "app_joint['credit_to_goods_ratio'] = app_joint['AMT_CREDIT'] / app_joint['AMT_GOODS_PRICE']\n",
    "app_joint['credit_to_income_ratio'] = app_joint['AMT_CREDIT'] / app_joint['AMT_INCOME_TOTAL']\n",
    "app_joint['days_employed_percentage'] = app_joint['DAYS_EMPLOYED'] / app_joint['DAYS_BIRTH']\n",
    "app_joint['income_credit_percentage'] = app_joint['AMT_INCOME_TOTAL'] / app_joint['AMT_CREDIT']\n",
    "app_joint['income_per_child'] = app_joint['AMT_INCOME_TOTAL'] / (1 + app_joint['CNT_CHILDREN'])\n",
    "app_joint['income_per_person'] = app_joint['AMT_INCOME_TOTAL'] / app_joint['CNT_FAM_MEMBERS']\n",
    "app_joint['payment_rate'] = app_joint['AMT_ANNUITY'] / app_joint['AMT_CREDIT']\n",
    "app_joint['phone_to_birth_ratio'] = app_joint['DAYS_LAST_PHONE_CHANGE'] / app_joint['DAYS_BIRTH']\n",
    "app_joint['phone_to_employ_ratio'] = app_joint['DAYS_LAST_PHONE_CHANGE'] / app_joint['DAYS_EMPLOYED']\n",
    "\n",
    "# Family members\n",
    "app_joint['cnt_non_child'] = app_joint['CNT_FAM_MEMBERS'] - app_joint['CNT_CHILDREN']\n",
    "app_joint['child_to_non_child_ratio'] = app_joint['CNT_CHILDREN'] / app_joint['cnt_non_child']\n",
    "app_joint['income_per_non_child'] = app_joint['AMT_INCOME_TOTAL'] / app_joint['cnt_non_child']\n",
    "app_joint['credit_per_person'] = app_joint['AMT_CREDIT'] / app_joint['CNT_FAM_MEMBERS']\n",
    "app_joint['credit_per_child'] = app_joint['AMT_CREDIT'] / (1 + app_joint['CNT_CHILDREN'])\n",
    "app_joint['credit_per_non_child'] = app_joint['AMT_CREDIT'] / app_joint['cnt_non_child']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: application_data\n",
       "  Entities:\n",
       "    apps [Rows: 600, Columns: 141]\n",
       "  Relationships:\n",
       "    No relationships"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize entityset\n",
    "es = ft.EntitySet('application_data')\n",
    "\n",
    "# add entities (application table itself)\n",
    "es.entity_from_dataframe(\n",
    "    entity_id='apps', # define entity id\n",
    "    dataframe=app_joint.drop('TARGET', axis=1), # select underlying data\n",
    "    index='SK_ID_CURR', # define unique index column\n",
    "    # specify some datatypes manually (if needed)\n",
    "    variable_types={\n",
    "        f: ft.variable_types.Categorical \n",
    "        for f in app_joint.columns if f.startswith('FLAG_')\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previous application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: application_data\n",
       "  Entities:\n",
       "    apps [Rows: 600, Columns: 141]\n",
       "    prev_apps [Rows: 410, Columns: 386]\n",
       "  Relationships:\n",
       "    prev_apps.SK_ID_CURR -> apps.SK_ID_CURR"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add entities (previous applications table)\n",
    "es = es.entity_from_dataframe(\n",
    "    entity_id = 'prev_apps', \n",
    "    dataframe = previous,\n",
    "    index = 'SK_ID_PREV',\n",
    "    time_index = 'DAYS_DECISION',\n",
    ")\n",
    "\n",
    "# add relationships\n",
    "r_app_cur_to_app_prev = ft.Relationship(\n",
    "    es['apps']['SK_ID_CURR'],\n",
    "    es['prev_apps']['SK_ID_CURR']\n",
    ")\n",
    "\n",
    "# Add the relationship to the entity set\n",
    "es = es.add_relationship(r_app_cur_to_app_prev)\n",
    "\n",
    "# check constructed entity set\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bureau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: application_data\n",
       "  Entities:\n",
       "    bureau [Rows: 2064, Columns: 83]\n",
       "    apps [Rows: 600, Columns: 141]\n",
       "    prev_apps [Rows: 410, Columns: 386]\n",
       "  Relationships:\n",
       "    prev_apps.SK_ID_CURR -> apps.SK_ID_CURR\n",
       "    bureau.SK_ID_CURR -> apps.SK_ID_CURR"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add entities (brueau table)\n",
    "es = es.entity_from_dataframe(\n",
    "    entity_id = 'bureau', \n",
    "    dataframe = bureau,\n",
    "    index = 'SK_ID_BUREAU',\n",
    "    time_index = 'DAYS_CREDIT'\n",
    ")\n",
    "\n",
    "# add relationships\n",
    "r_app_cur_to_bureau = ft.Relationship(\n",
    "    es['apps']['SK_ID_CURR'],\n",
    "    es['bureau']['SK_ID_CURR']\n",
    ")\n",
    "\n",
    "# Add the relationship to the entity set\n",
    "es = es.add_relationship(r_app_cur_to_bureau)\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 395 ms, sys: 20.1 ms, total: 415 ms\n",
      "Wall time: 381 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define cut-off times\n",
    "# cut-off times are the \"right\" time values to be used for feature calculation without future leaks\n",
    "# none in our case\n",
    "\n",
    "cutoff_times = pd.DataFrame(app_joint.SK_ID_CURR)\n",
    "cutoff_times['time'] = today\n",
    "\n",
    "# add last_time_index\n",
    "es.add_last_time_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_matrix(fm_sum_, fm_, period, parent_cols):\n",
    "  child_cols = fm_.drop(columns = parent_cols).columns\n",
    "  fm_ = fm_[child_cols].add_prefix(str(period) + \" MONTHS(\").add_suffix(\")\")\n",
    "  return fm_sum_.join(fm_)\n",
    "  \n",
    "def period_matrix(period, es = es, target_entity = 'prev_apps'):\n",
    "  # Specify the fm with the first period\n",
    "  fm_sum, feature_names = ft.dfs(entityset = es, target_entity = target_entity,\n",
    "                                 drop_contains = ['SK_ID_CURR','SK_ID_PREV','SK_ID_BUREAU'],\n",
    "                                 agg_primitives=[\n",
    "                                   'mean',\n",
    "                                   'std',\n",
    "                                   'median',\n",
    "                                   'min',\n",
    "                                   'max',\n",
    "                                   'count',\n",
    "                                   'last',\n",
    "                                   'num_unique'\n",
    "                                 ], \n",
    "                                 trans_primitives=[\n",
    "                                   'time_since_previous',\n",
    "                                   'weekday',\n",
    "                                   'month',\n",
    "                                   'year'\n",
    "                                 ],\n",
    "                                 max_depth=2,\n",
    "                                 cutoff_time=cutoff_times,\n",
    "                                 training_window=ft.Timedelta(period[0]*31, \"d\"), # use only last X days in computations\n",
    "                                 max_features=3000,\n",
    "                                 chunk_size=10000,\n",
    "                                 verbose=True\n",
    "                                )\n",
    "  parent_cols = [col for col in fm_sum.columns \n",
    "                 if (('(cash.' not in col) & ('(installments.' not in col) & ('(credit.' not in col)\n",
    "                     & ('(bureau_balance.' not in col) & ('(bureau.' not in col) & ('(prev_apps.' not in col))]\n",
    "  fm_sum = merge_matrix(fm_sum[parent_cols], fm_sum, period[0], parent_cols)\n",
    "  \n",
    "  for i in period[1:]:\n",
    "    # DFS with specified primitives\n",
    "    fm, feature_names = ft.dfs(entityset = es, target_entity = target_entity,\n",
    "                           drop_contains = ['SK_ID_CURR','SK_ID_PREV','SK_ID_BUREAU'],\n",
    "                           agg_primitives=[\n",
    "                             'mean',\n",
    "                             'std',\n",
    "                             'median',\n",
    "                             'min',\n",
    "                             'max',\n",
    "                             'count',\n",
    "                             'last',\n",
    "                             'num_unique'\n",
    "                           ], \n",
    "                           trans_primitives=[\n",
    "                             'time_since_previous',\n",
    "                             'weekday',\n",
    "                             'month',\n",
    "                             'year'\n",
    "                           ],\n",
    "                           max_depth=2,\n",
    "                           cutoff_time=cutoff_times,\n",
    "                           training_window=ft.Timedelta(31*i, \"d\"), # use only last X days in computations\n",
    "                           max_features=3000,\n",
    "                           chunk_size=10000,\n",
    "                           verbose=True\n",
    "                          )\n",
    "    fm_sum = merge_matrix(fm_sum, fm, i, parent_cols)\n",
    "    print('%d Total Features' % len(feature_names))\n",
    "  \n",
    "  # Merge with the whole training time matrix\n",
    "  fm, feature_names = ft.dfs(entityset = es, target_entity = target_entity,\n",
    "                       drop_contains = ['SK_ID_CURR','SK_ID_PREV','SK_ID_BUREAU'],\n",
    "                       agg_primitives=[\n",
    "                         'mean',\n",
    "                         'std',\n",
    "                         'median',\n",
    "                         'min',\n",
    "                         'max',\n",
    "                         'count',\n",
    "                         'last',\n",
    "                         'num_unique'\n",
    "                       ], \n",
    "                       trans_primitives=[\n",
    "                         'time_since_previous',\n",
    "                         'weekday',\n",
    "                         'month',\n",
    "                         'year'\n",
    "                       ],\n",
    "                       max_depth=2,\n",
    "                       cutoff_time=cutoff_times,\n",
    "                       #training_window=ft.Timedelta(31*i, \"d\"), # use only last X days in computations\n",
    "                       max_features=3000,\n",
    "                       chunk_size=10000,\n",
    "                       verbose=True\n",
    "                      )\n",
    "  fm = fm.drop(columns = parent_cols)\n",
    "  fm_sum = fm_sum.join(fm)\n",
    "  return fm_sum, fm_sum.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 2926 features\n",
      "Elapsed: 00:15 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 1/1 chunks\n",
      "Built 2926 features\n",
      "Elapsed: 00:35 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 1/1 chunks\n",
      "2926 Total Features\n",
      "Built 2926 features\n",
      "Elapsed: 00:36 | Remaining: 00:00 | Progress: 100%|██████████| Calculated: 1/1 chunks\n"
     ]
    }
   ],
   "source": [
    "fm, features_names = period_matrix(period = [6, 36], es = es, target_entity = 'apps')\n",
    "#features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [col for col in fm.columns if '(last(' in col]\n",
    "fm = fm.drop(columns = drop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the generated feature_matrix to the original dataframe\n",
    "app_joint = app_joint.merge(fm.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_joint = app_joint.loc[:,app_joint.columns != 'TARGET'].fillna(0).join(app_joint['TARGET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_joint.to_csv('Feature_Matrix_app_joint.csv', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
